# Design Choices
We started our agents as a single offensive agent and a single defensive agent similar to baselineTeam’s. This however did not prove efficient based on the contest feedback, and we found that both staff_team_medium and staff_team_top use two offensive agents concurrently, proving that our single defensive agent may not have the capability to defend our side of the grid.

We then switched our offensive agent to have both attack and defensive capabilities, which would help if two opponent agents were to attack. However this soon produces another issue as both agents would chase after a single opposing agent and leave the opponent’s other agent unsupervised.

We changed our approach and had two offensive agents that are capable of attacking and defending at the same time, each governing ‘top’ and ‘bottom’ part of the grid concurrently for the preliminary contest, this however didn’t work correctly as our agents would tend to get stuck in a loop and would chase enemies on our end of the grid but never actually eat them due to incorrect weights set, always keeping 1 cell between them.

We attempted to use a Q-learning agent as the base agent rather than a reflex agent, so that it could compare legal actions using Q-values. However, because the desired features and their weights were already refined enough, Q-learning was taking steps in the wrong direction. We attempted to make an deep Q-learning agent, but there wasn't enough time to complete work on that agent.

After trying to achieve good results by improving the expectimax agents further and not getting much of anything out of it, we decided to pivot to a new strategy using Monte Carlo Technique, which gave us exceedingly great results compared to our previous attempts. We decided to use these agents as our final submission after improving their capabilities. We have also tried to combine Q-learning strategies together with our Monte Carlo Technique, however the result it gave was not as good as the result we originally got from the feedback contests.

## General Comments
Coding Pacman is really fun and interesting, however it is not easy. We often struggle to get it right. A little tweak in the code could result in a huge difference in Pacman's behaviour, so it took us a relatively long to fix up the weird behaviour. Understanding each of the concepts in lectures and implementing it in this assignment sure is tough for us but we have learnt a great deal based on this assignment, especially about how expectimax, Q-learning, as well as Monte Carlo Techniques works.

## Comments per topic

## Offense
For offense, we have tried multiple AI techniques for this agent. We started off by using expectimax technique. This technique was working alright however it wasn't smart a lot of the time. Our Pacman is not able to fully detect the opponent behind walls until they are close enough. We improved the weights and features function, and this brought about some interesting behaviour from Pacman, as most of the time it would prioritise stealing food from the opponent's grid. However, at times when the weights of 'ghostScared' are higher, Pacman will exert a more cowardly behaviour and often led to a loss. We switched the base reflexCaptureAgent to QlearningAgent, and this technique works pretty well as it computes a better action with Q-values than the basic reflexCaptureAgent. However, we were still unable to beat team_medium despite implementing Q-learning as each map in the competition could have a different map layout and we would have trouble training our agent to be efficient on every map. Finally, we found a better solution to Q-learning. We have implemented a Monte Carlo strategy. We have conducted lots of research on this strategy and have decided to give it a go. After refining the Monte Carlo technique, we submitted it in a round of feedback contest and it gave us an extremely promising result, so we have move forward in using the Monte Carlo Technique in our final submission. We have also implemented multiple actions functions instead of just one 'chooseAction' to further improve the decision making of our Pacman. We are still a long way away from beating team_super, however based on the results we're getting from feedback, we think it is still quite good.
## Defense
Defense agent was quite similar to offensive. We started off by using expectimax agent, however it wasn't smart as it could not sense the opponent at the bottom part of the grid. We often lost to staff_team_medium and staff_team_top with this approach as our agent was often overwhelmed by the opponent's team. We theorised that a single defense agent was not capable of defending our grid, so we changed our agent to be able to handle both offense and defense. We switched both agents to this strategy, but with one patrolling the top part of the grid, and the other the bottom part of the grid. This was ineffective as well when we were put up against staff_team_medium and staff_team_top, as both of our agents would often chase after a single opponent and leave the other unsupervised. We moved to our final approach by reverting back to a single offense agent and a single defense agent. Our current defense agent works by supervising the borders of the grid. We did this by computing the map layout and setting the defense agent a path to the borders. Now our defense agent is relatively efficient in hunting down the opponent's Pacman as well as patrolling borders of our side of the grid.
