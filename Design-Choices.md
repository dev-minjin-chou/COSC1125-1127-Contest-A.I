# Design Choices
We started our agents as a single offensive agent and a single defensive agent similar to baselineTeam’s. This however did not prove efficient based on the contest feedback, and we found that both staff_team_medium and staff_team_top use two offensive agents concurrently, proving that our single defensive agent may not have the capability to defend our side of the grid.

We then switched our offensive agent to have both attack and defensive capabilities, which would help if two opponent agents were to attack. However this soon produces another issue as both agents would chase after a single opposing agent and leave the opponent’s other agent unsupervised.

We changed our approach and had two offensive agents that are capable of attacking and defending at the same time, but at the ‘top’ and ‘bottom’ part of the grid for the preliminary contest, this however didn’t work correctly as our agents would tend to get stuck in a loop and would chase enemies on our end of the grid but never actually eat them, always keeping 1 cell between them.

We attempted to use a Q-learning agent as the base agent rather than a reflex agent, so that it could compare legal actions using Q-values. However, because the desired features and their weights were already refined enough, Q-learning was taking steps in the wrong direction. We attempted to make an approximate Q-learning agent, but there wasn't enough time to complete work on that agent.

After trying to achieve good results by improving the expectimax agents further and not getting much of anything out of it, we decided to pivot to a new strategy using Monte Carlo Technique, which gave us exceedingly great results compared to our previous attempts. We decided to use these agents as our final submission after improving their capabilities. We have also tried to combine qlearning strategies together with our monte carlo technique however the result it gave is not as good as the result we originally got from the feedback contests.

## General Comments
Coding the pacman is really fun and interesting however it is not easy. We often struggle to get it right occasionally. A little tweak in the code could result in a huge difference in the pacman's behaviour therefore it took us relatively long to fix up the weird behaviour it firstly exerts.

## Comments per topic

## Offense
For offense we have tried multiple AI techniques for this agent. We started off by using expectimax technique. This technique is working alright however it is not smart at times. Our pacman is not able to fully detect the opponent behind the walls until they are close enough. We soon improved the weights and features function, this brought some interesting behaviour of pacman as most of the time it will prioritise stealing food from opponent's grid however at times when the weights of 'ghostScared' are higher, pacman will result in a more cowardly behaviour and often leads to a 'distressing' lost. We have then switched the base reflexCaptureAgent to QlearningAgent, this technique works pretty well due to it computes a better action with qvalues than the basic reflexCaptureAgent. However, we are still unable to beat team_medium despite implementing qlearning as due to it is only approximate qlearning and each maps tested have a different map layout we have trouble training our agent to be efficient in every maps. Finally we found a better solution to qlearning. We have implemented a MonteCarlo strategy. We have conducted multiple research on this strategy and have decided to give it a go. After refining the monteCarlo technique, we have submitted it in a round of feedback contest and it gave us an extremely promising result therefore we have move forward in using montecarlo as our final submission technique. We have also implemented multiple actions functions instead of just one 'chooseAction' to further improve the decision making of our pacman. We are still long way away from beating team_super however based on the results we're getting from feedback we think it is good enough. 
## Defense
Defense agent on the other hand was quite similar to offensive. We started off by using expectimax agent however it is not smart as it does not sense the opponent at the bottom part of the grid. We often lost to team_medium and team_top with this approach due to our agent are often overwhelmed by the opponent's team. We found out that a single defense agent is not capable of defending our grid therefore we have switched our agent to be able to proceed with both offense and defense. We have switched both agents to this but with one governing the top part of the grid and the other the bottom part of the grid. This is ineffective as well when we were put up against staff_teams medium and top due to both of our agents would often chase after a single opponent and left the other unsupervised. We have then move to our final approach by switching it to a single offense agent and a single defense agent. Our current defense agent works by supervising the borders of the grid. We did this by computing the map layout and setting the defense agent a path to the borders. Now our defense agent is relatively efficient in hunting down the opponent's pacman as well as governing borders of our side of the grid.
